<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[第一个Scrapy项目]]></title>
    <url>%2FFirstScrapyproject.html</url>
    <content type="text"><![CDATA[一个Scrapy项目123$ scrapy startproject projectname$ cd projectname$ tree 声明item列出需要爬取的字段列表,再去修改scrapy为我们创建的ProjectsItem类,就变的很容易,修改projectname/item.py文件:12345from scrapy import Item, Fieldclass ProjectnameItem(Item): title=Field() url=field() 编写爬虫通常,我们会为每个网站或网站的一部分(如果网站非常大的话)创建一个爬虫,爬虫代码实现了完整的UR2IM流程.什么时候用爬虫,什么时候用项目呢？项目是由若干爬虫组成的，如果用很多网站，并且需要从中抽取相同类型的Item，比如：房产，那么这些网站可以用同一个项目，并且为每个源/网站使用一个爬虫。反之，如果要处理图书和房产两个不同的源时，则应该使用不同的项目。可以在文本编辑器中从头开始创建一个爬虫，不过，为了减少输入，更好的方法是使用scrapy genspider命令，下面两种写法都可以123456789101112131415161718192021222324252627#第一种写法import scrapyclass BasicSpider(scrapy.Spider) name=&quot;basic&quot; #爬虫的name必须是唯一值 start_urls=&#123; &apos;http://web.com&apos;, &#125; def parse(self,response): pass# 第二种写法import scrapyclass BasicSpider(scrapy.Spider) name=&quot;basic&quot; #爬虫的name必须是唯一值 start_urls=&#123; &apos;http://example.com&apos;, &#125; def start_request(self): urls=&#123; &apos;http://example.com&apos;, &#125; for url in urls: scrapy.Request(url,callback=self.parse) # callback为空时默认为self.parse def parse(self,response): pass import 语句能让我们使用Scrapy框架中已有的类，下面是扩展自scrapy.Spider的BasicSpider类的定义。通过“扩展”的方式，尽管我们没有写任何代码，但实际上已经继承了Scrapy框架中的Spider类的相当一部分功能。这样，就可以值额外编写少量代码，就能获得一个完整运行的爬虫了。 填充item首先需要引入ProjectnameItem类。如前所述，它在projectname目录的items.py文件中，也就是projectname.items模块中，在basic.py中引如该模块.from projectname.items import ProjectnameItems然后需要实例化并返回一个对象.这非常简单.在parse方法中,可以通过添加items=ProjectnameItem()语句创建一个新的item,然后可以按如下的方式为其字段分配表达式.item[&quot;title&#39;]=response.xpath().extract()最后通过return items返回items通过命令行scrapy crawl baisc运行爬虫 保存文件1234$ scrapy crawl basic -o items.json$ scrapy crawl basic -o items.csv$ scrapy crawl basic -o itemsjlscrapy crawl basic -o items.jl 我们不需要编写任何额外的代码,,就可以保存为这些不同的格式.csv和xml非常流行因为类似微软Excel的程序可以直接打开它们.JSON文件在网上非常流行,原因是它们富有表现力而且与js的关系非常的密切,JSON与JSON Line(jl)格式的轻微不同是,囧on文件是在大数组中储存json对象的,而.jl文件则是每行包含一个JSON对象,所以可以被更高效的读取. 将你生成的文件保存到文件系统之外的地方也很容易,$ scrapy crawl basic -o ftp://user:pass@ftp.scrapybook.com/item.json$ scrapy crawl basic -o s3://aws_key:aws_secret@s3.scrapybook.com/item.json可以直接使用scrapy parsescrapy parse --spider=basic http://example.com 清理–item装载器与管理字段首先,我们使用一个强大的工具类–ItemLoader,以代替那些繁杂的extract()和xpath()操作. 123456789101112from scrapy.Spiderfrom scrapy.loader import ItemLoaderfrom myproject.items import Productdef parse(self, response): l = ItemLoader(item=Product(), response=response) l.add_xpath(&apos;name&apos;, &apos;//div[@class=&quot;product_name&quot;]&apos;) l.add_xpath(&apos;name&apos;, &apos;//div[@class=&quot;product_title&quot;]&apos;) l.add_xpath(&apos;price&apos;, &apos;//p[@id=&quot;price&quot;]&apos;) l.add_css(&apos;stock&apos;, &apos;p#stock]&apos;) l.add_value(&apos;last_updated&apos;, &apos;today&apos;) # you can also use literal values return l.load_item() 文档地址:https://doc.scrapy.org/en/latest/topics/loaders.html这种写法不止是在视觉上更加舒服，它还非常明确地声明了我们意图去做的事情，而不会将其与实现细节混淆起来。使得代码具有更好的可维护性以及自描述性。Itemloaders通过不同的处理类（processors）传递XPath/CSS表达式的值。处理器是一个快速而又简单的函数。处理器的一个例子就是Join()。该处理其可以把多个结果连在一起。另一个有意思的处理器是MapCompose(),使用该处理器,你可以使用任意的Python函数或函数链，以实现复杂的功能， 创建contractcontract有点像为爬虫设计的单元测试。他可以让你快速知道哪里有运行异常。contract包含在紧挨着函数名的注释中，并以@开头。1234567&apos;&apos;&apos;This Function parses a properties page. contracks @url http://www.bookschina.com/kinder/27000000/ @returns items 1 @scrapes title sellPrice description publisher author image_urls @scrapes url project spider server date &apos;&apos;&apos; 抽取更多的URL 横向——从一个索引页到另一个索引页 纵向——从一个索引页到详情页面我们将前者称为水平爬取，后者称为垂直爬取使用爬虫实现双向爬取现在我们编写一个新的parse（）方法，来实现水平和垂直两种抓取方式 12345678910def parse(self,response): #Get the next index URLs and yield Request next_selector=response.xpath(&apos;//*[contains(@class,&quot;next&quot;)]/@href) for url in next_selector.extract(): yield scrapy.Request(response.urljoin(url)) #Get item URLs and yield Request Item_selector=response.xpath(&apos;//*[contains(@itemprop,&quot;url&quot;)]/@href) for url in Item_selector: scrapy.Request(response.urljoin(url),callback=self.parse_item) 我们现在已经准备好运行该爬虫了。不过要让该爬虫以当前的方式运行的话，则会抓取网站完整的几万个页面。为了避免运行时间过长，课通过命令行参数：-s CLOSESPIDER_ITEMCOUTTN=90告知爬虫在爬取指定数量（90）的Item后停止运行。 使用CrawlSpider 实现双向爬取如果感觉上面的双向爬取有些冗长，则说你确实发现了关键的问题。Scrapy尝试简化此类通用的情况，以使编码更加简单。最简单的实现同样结果的方式是使用CrawlSpider，这是一个能够更容易实现这种爬虫的类。为了实现它，我们需要使用genspider命令，并设置-t crawl参数，以使用crawl爬虫模板创建一个爬虫。$ scrapy genspider -t crawl easy web.com现在，爬虫文件easy.py包含如下内容：1234567891011121314151617181920# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleclass EasySpider(CrawlSpider): name = &apos;easy&apos; allowed_domains = [&apos;web.com&apos;] start_urls = [&apos;http://web.com/&apos;] rules = ( Rule(LinkExtractor(allow=r&apos;Items/&apos;), callback=&apos;parse_item&apos;, follow=True), ) def parse_item(self, response): i = &#123;&#125; #i[&apos;domain_id&apos;] = response.xpath(&apos;//input[@id=&quot;sid&quot;]/@value&apos;).extract() #i[&apos;name&apos;] = response.xpath(&apos;//div[@id=&quot;name&quot;]&apos;).extract() #i[&apos;description&apos;] = response.xpath(&apos;//div[@id=&quot;description&quot;]&apos;).extract() return i 当你阅读这段自动生成的代码时,你会发现它和之前的爬虫有一些相似,不过在此处的生命==类声明中,会发现爬虫时继承自CrawlSpider,而不再时Spider.CrawlSpider提供了以个使用rules变量实现的parse()方法,这和我们之前例子中手工实现的功能一致.现在,我们要把start_urls 设置成第一个索引页,并使用我们之前的实现替换预定义的parse_item()方法.这次我们将不再需要实现任何parse()方法.我们将预定义的rules变量替换为两条规则,即水平抓取和垂直抓取.12345ruls = (Rule(LinkExtractor(restrict_xpath=&apos;//*[contains(@class,&quot;next&quot;)]&apos;)),Rule(LinkExtractor(restrict_xpath=&apos;//*[@itemprop=&quot;url&quot;]&apos;), callback=&apos;parse_item&apos;)) 这两条规则使用的是和我们之前手工实现的示例中相同的XPath表达式,不过这里没有了a或是href的限制.顾名思义,LinkExtractor正是专门用于抽取链接的,因此在默认情况下,他们会去查找a(及area)href属性.你可以通过设置LinkExtractor()的tags和attrs参数来进行自定义.需要注意的是,回调参数目前包含的是回调方法名称的字符串(比图’parse_item’),而不是方法引用,如Request(self.parse_Item).最后,除非设置了callback参数,否则Rule将跟踪已经抽取的URL,也就是说他将会扫描目标页面以获得额外的链接并跟踪它们,.如果设置课callback,Rule将不会跟踪目标页面的链接.如果你希望它跟踪链接,应当在callback方法中使用return或yield返回它们,或者将Rule()的follow参数设置为True.]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>python</tag>
        <tag>爬虫，Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬取小说《心里罪城市之光》]]></title>
    <url>%2FPython%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4%E3%80%8A%E5%BF%83%E7%90%86%E7%BD%AA%E5%9F%8E%E5%B8%82%E4%B9%8B%E5%85%89%E3%80%8B.html</url>
    <content type="text"><![CDATA[我们选择努努小说进行小说的爬取http://www.kanunu8.com/book4/10571/ 先写代码将目录爬下来，添加到一个列表中，比较简单，在此就不多说123456789101112131415161718192021222324252627282930#爬取结果title=[&apos;序 往事&apos;, &apos;第一章 和自己赛跑的人&apos;, &apos;第二章 求婚&apos;, &apos;第三章 报应&apos;, &apos;第四章 足迹&apos;, &apos;第五章 回忆的灰烬&apos;, &apos;第六章 子宫&apos;, &apos;第七章 雨夜寻踪&apos;, &apos;第八章 噩梦&apos;, &apos;第九章 对手&apos;, &apos;第十章 思路&apos;, &apos;第十一章 同态复仇&apos;, &apos;第十二章 他的样子&apos;, &apos;第十三章 地下室&apos;, &apos;第十四章 似曾相识&apos;, &apos;第十五章 城市之光&apos;, &apos;第十六章 死期&apos;, &apos;第十七章 公决&apos;, &apos;第十八章 掌印&apos;, &apos;第十九章 老宅&apos;, &apos;第二十章 身份&apos;, &apos;第二十一章 轮回&apos;, &apos;第二十二章 杀手养成&apos;, &apos;第二十三章 最爱&apos;, &apos;第二十四章 忽略&apos;, &apos;第二十五章 夺走&apos;, &apos;第二十六章 熄灭&apos;, &apos;第二十七章 死者的证言&apos;, &apos;尾声 我想你要走了&apos;] 打开几章小说，观察URL规律~http://www.kanunu8.com/book4/10571/186030.html~~http://www.kanunu8.com/book4/10571/186031.html~ 1234i=30while i&lt;59: url=&apos;http://www.kanunu8.com/book4/10571/1860&apos;+str(i)+&apos;.html&apos; i+=1 用循环打开并爬取每个网页（每个章节）使用框架打开网页： 123456try: r=requests.get(url,timeout=30) r.raise_for_status() #判断网页返回的状态码是否为200 r.encoding=r.apparent_encoding except: return &apos;产生异常&apos; 由于网页结构比较简单，使用Beautiful Soup解析网页内容 12soup=BeautifulSoup(r.text,&apos;lxml&apos;)tag=soup.p.contents 将解析的小说内容写入到文件 12345678path=&apos;g:/&apos;+title[(i-30)]+&apos;.txt&apos; with open(path,&apos;w&apos;) as f: #f.write(&apos;第 %s 章\r\n\r\n&apos;%(i-31)) for a in tag: if type(a) is bs4.element.NavigableString: f.write(str(a.string[6:])+&apos;\n&apos;) print(a.string[6:]) f.close() ###完整代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# -*- coding: utf-8 -*-&quot;&quot;&quot;Comments parsing @author: NiceBlueChai&quot;&quot;&quot; import requestsimport bs4from bs4 import BeautifulSoupimport timedef getHTMLText(url): try: r=requests.get(url,timeout=30) r.raise_for_status() r.encoding=r.apparent_encoding return r.text except: return &apos;产生异常&apos;title=[&apos;序 往事&apos;, &apos;第一章 和自己赛跑的人&apos;, &apos;第二章 求婚&apos;, &apos;第三章 报应&apos;, &apos;第四章 足迹&apos;, &apos;第五章 回忆的灰烬&apos;, &apos;第六章 子宫&apos;, &apos;第七章 雨夜寻踪&apos;, &apos;第八章 噩梦&apos;, &apos;第九章 对手&apos;, &apos;第十章 思路&apos;, &apos;第十一章 同态复仇&apos;, &apos;第十二章 他的样子&apos;, &apos;第十三章 地下室&apos;, &apos;第十四章 似曾相识&apos;, &apos;第十五章 城市之光&apos;, &apos;第十六章 死期&apos;, &apos;第十七章 公决&apos;, &apos;第十八章 掌印&apos;, &apos;第十九章 老宅&apos;, &apos;第二十章 身份&apos;, &apos;第二十一章 轮回&apos;, &apos;第二十二章 杀手养成&apos;, &apos;第二十三章 最爱&apos;, &apos;第二十四章 忽略&apos;, &apos;第二十五章 夺走&apos;, &apos;第二十六章 熄灭&apos;, &apos;第二十七章 死者的证言&apos;, &apos;尾声 我想你要走了&apos;]for i in range(30,58): path=&apos;g:/&apos;+title[(i-30)]+&apos;.txt&apos; url=&apos;http://www.kanunu8.com/book4/10571/1860&apos;+str(i)+&apos;.html&apos; try: r=requests.get(url,timeout=30) r.raise_for_status() r.encoding=&apos;GBK&apos; except: print(&apos;异常！！！&apos;) soup=BeautifulSoup(r.text,&apos;lxml&apos;) tag=soup.p.contents with open(path,&apos;w&apos;) as f: #f.write(&apos;第 %s 章\r\n\r\n&apos;%(i-31)) for a in tag: if type(a) is bs4.element.NavigableString: f.write(str(a.string[6:])+&apos;\n&apos;) print(a.string[6:]) f.close() time.sleep(1) ❤️我的目标是：someday，即便你花钱看我的文章，也会觉得心满意足]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Windows-Server上搭建Web服务器]]></title>
    <url>%2F%E5%9C%A8Windows-Sever-%E4%B8%8A%E6%90%AD%E5%BB%BAWeb%E6%9C%8D%E5%8A%A1%E5%99%A8.html</url>
    <content type="text"><![CDATA[两种方法： 安装IIS 安装Apache 安装IIS作为web服务器要求 安装WindowsSever系统（可使用虚拟机，要开启NAT服务） 将IP地址设置为静态的 计算机可以访问Internet+开放80端口，可用cmd命令netstat -an查看 注意：虚拟机上本地链接中网关要与虚拟机编辑-&gt;NAT服务中一致 在WindowsSever上安装IIS服务器管理 -&gt; 添加角色，按步骤安装安装后应有以下服务启动 打开后有一个默认网站 安装DNS服务，搭建DNS服务器添加正向查找区域访问时将本地连接属性中IPv4属性的DNS该为DNS服务器的ipv4地址使用域名访问时便可以解析到指定IP地址 限制网站宽带和连接数量一个服务器上可以放许多网站，而服务器的带宽是一定的为了保证服务器的性能与正常工作、每个网站的正常访问、每个用户的体验，需要对每个网站的带宽和网站并发连接数进行限制 右击 管理网站 高级设置 网站重定向简单的说，自动跳转网站与网页都可以重定向可以重定向到其他网站，也可以重定向到本站的其他页面和文件 定向到media 定向到百度 网站访问日志每次访问网站都会在日志中记录，可以在选项中选择记录项，记录的日志默认保存在…\inetpub\logs\LogFiles目录下 网站标识可在一个服务器上创建多个网站，区分他们的方法，使用网站标识： web服务器网卡绑定多的IP地址，每个网站使用不同的IP地址，太奢侈，因为公网IP资源紧张，不采用此方法 通过端口访问不同的网站，80端口为默认端口 绑定域名，通过域名访问网站 配置网站支持文件下载仅需要80端口在网站物理目录下新建一个media文件夹，放一些东西，访问该目录 要访问文件需要开启目录浏览 再次访问 要想让别人下载各种拓展名（格式）的文件需要配置MIME类型 保持会话连接网站使用TCP协议，每次连接建立会话需要三次握手，为了提高效率，一次建立会话后连接保持一定时间，不许要每次请求都进行三次握手浪费时间，访问量较大时保持连接很耗费服务器性能，取消连接超时则耗费带宽，根据情况选择。 MC地址与ARP欺骗网络执法官（控制本网段计算机的通信）、P2P终结者（控制本网段计算机上网带宽）这两个（流氓）软件通过arp欺骗进行“上网管理”，还可以通过arp欺骗捕获本网段计算机的账号密码，arp协议已在IPv6中停用 判断是否被arp欺骗当网络连接正常，但无法上网，而同网段其他计算机可以上网，可能是计算机受到攻击中病毒了，可通过命令行cmd查看arp -a,对比同网段其他计算机DNS缓存的MC地址，若不一样就是被ARP欺骗了。 解决方法 可通过命令行指令arp -s [ip] [mc]来设置dns服务器mc地址 可以通过修复网卡（修复网络连接）清除缓存的MC地址 注：只有使用IPv4协议并且使用**自动获取IP、自动获取DNS的计算机会遭到ARP欺骗❤️]]></content>
      <categories>
        <category>服务器</category>
        <category>Web服务器</category>
      </categories>
      <tags>
        <tag>Windows-Server</tag>
        <tag>搭建Web服务器</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt-QLabe显示文字和图片]]></title>
    <url>%2FQt-QLabe%E6%98%BE%E7%A4%BA%E6%96%87%E5%AD%97%E5%92%8C%E5%9B%BE%E7%89%87.html</url>
    <content type="text"><![CDATA[QLabe Class参考文档 文字与图片的显示格式实际上是用Qt StyleSheet（样式表）设置的与css样式中的差不多 对齐方式 是否换行 设置边框 显示LOGO(显示图片) 添加图片资源 添加一个Labe用来显示图片 ❤️]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>QLabe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[定时器Timer]]></title>
    <url>%2F%E5%AE%9A%E6%97%B6%E5%99%A8Timer.html</url>
    <content type="text"><![CDATA[定时器用于实现定时操作，如： 每三分钟保存一次 每500毫秒闪烁一次 每1秒刷新一次界面 新建Timer (1) 重写timeEvent()函数virtual void timerEvent ( QTimerEvent * event );123456789// 定时器处理函数void Test8_2A::timerEvent ( QTimerEvent * event )&#123;// 可以有多个定时器，每个的定时器有不同的处理 if(event‐&gt;timerId() == m_timerId) &#123; &#125;&#125; (2) 启动定时器，指定时间间隔m_timerId=startTime(500); 例子：实现一个文字时钟，显示系统当前时间 1234567891011121314151617Timer_Event::Timer_Event(QWidget *parent) : QMainWindow(parent)&#123; ui.setupUi(this); m_timerId = startTimer(1000);&#125;void Timer_Event::timerEvent(QTimerEvent *event)&#123; if (event-&gt;timerId()==m_timerId) &#123; QTime now = QTime::currentTime(); QString text = now.toString(&quot;HH:mm:ss ap&quot;); ui.labelTime-&gt;setText(text); &#125; &#125; ❤️]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>Timer</tag>
        <tag>定时器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt-使用工作线程]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8%E5%B7%A5%E4%BD%9C%E7%BA%BF%E7%A8%8B.html</url>
    <content type="text"><![CDATA[comments: false&gt;在Qt里，线程相关的类： QThread 线程 QMutex 互斥锁QSemaphore 信号量 “工作线程”:就是一个普通的线程,区别于”界面线程”当一个事件处理可能需要较长时间,就创建一个工作线程处理它,以避免界面卡死. 步骤:(1) 启动一个工作线程(2) 显示一个进度条或对等待话框(3) 启动一个定时器,定时查询工作线程的进度和状态,在其完成之后,填满进度条、结束等待. 创建线程派生一个线程类12345678class MyThread : public QThread&#123;Q_OBJECTpublic:MyThread(QObject *parent);~MyThread();void run(); // 线程的入口函数&#125;; 添加一些成员函数，用于查询任务的状态和进度 :状态：int GetStatus();已完成(1)正在进行(0)发生错误，已终止(‐1)进度：int GetProgress();0‐100 启动线程12MyTask* m_task = new MyTask();m_task‐&gt;start(); 回收线程1m_task-&gt;wait();]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>线程</tag>
        <tag>工作线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt的信号和槽]]></title>
    <url>%2FQt%E7%9A%84%E4%BF%A1%E5%8F%B7%E5%92%8C%E6%A7%BD.html</url>
    <content type="text"><![CDATA[信号和槽是Qt特有的信息传输驱动机制，是Qt程序设计中的重要基础，它可以让程序员在互不相干的对象之间建立联系。 槽本质上是类的成员函数，其参数可以是任意类型，它与普通的C++成员函数几乎没有区别，它可以是虚函数；也可以被重载函数；可以是公有的、保护的或私有的，也可以被其它C++成员函数调用，和普通的类成员函数唯一不同的是：槽还可以和信号连接在一起，每当和槽连接的信号被发射时，就会自动调用这个槽。 connect()语法形式如下：connect(sender,SIGNAL(signal),receiver,SLOT(slot));这里的sender是发送信号的对象的指针，receiver是包含槽的对象的指针，signal是被发送的信号，slot是接受信号的槽，他们都是不带参数的函数名。SIGNAL()宏和SLOT()宏会把他们的参数转换成相应的字符串。对于信号和槽，还具有以下特点。 1.一个信号可以连接多个槽12connect(sender,SIGNAL(signal),receiver,SLOT(slotA));connect(sender,SIGNAL(signal),receiver,SLOT(slotB)); 当这一信号发射时，会以不确定的顺序一个接一个的调用这个信号连接的所有槽 2.多个信号可以连接同一个槽12connect(sender,SIGNAL(signalA),receiver,SLOT(slot));connect(sender,SIGNAL(signalB),receiver,SLOT(slot)); 无论哪个信号被发射，这个槽都会被调用 3.一个信号可以连接另外一个信号1connect(sender,SIGNAL(signalA),receiver,SIGNAL(signalB)); 当信号A发射时，也会发射与它相连的信号B ４.信号、槽之间的连接可以被移除1disconnect(sender,SIGNAL(signal),receiver,SLOT(slot)); 因为删除对象时，Qt会自动移除和这个对象相关的所有连接，所以这个语法很少用到。 除此之外，要把信号成功连接到槽（或者连接到另一个信号），还应该注意，相连接的信号和槽必须具有相同顺序相同类型的参数，如果信号的参数比它所连接的槽的参数多，那么多余的参数将被简单的忽略掉。例如：12connect(sender,SIGNAL(rawCommandReply(int,const QString &amp;)),receiver,SIGNAL(processReply(int , const QString &amp;))); 如果参数类型不匹配，或者信号或槽不存在，尽管应用程序调试构建可能会通过，但Qt会在运行时发出警告。如果信号和槽的名字中包含了参数名，也会发生错误警告 实际上这种机制是在QObject中实现的，这表明信号和槽并不仅仅局限于GUI编程中，实际上这种机制可以用于任何QObject的子类中， 在普通的类中使用信号和槽机制123456789101112131415161718192021Class Circle:public QObject&#123; Q_ONJECT public: Circle()&#123;CircleRadius=0;&#125; int Radius()&#123;return CircleRadius;&#125; public slots: void setRadius(int newRadius); signals: void radiusChanged(int newRadius); private: int circleRadius;&#125;;void Circle::setRadius(int newRadius)&#123; if(newRadius!=circleRadius) &#123; circleRadius=newRadius; emit radiusChanged(circleRadius); &#125;&#125; 来看一下，setRadius()槽是如何工作的。仔细阅读会发现只有在newRadius不等于circleRadius的时候，才会发射radiusChanged()信号。这样既可以确保信号和槽连接又不会导致无限循环。 ❤️我的目标是：someday，即便你花钱看我的文章，也会觉得心满意足]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>信号和槽</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt-非模式对话框]]></title>
    <url>%2FQt-%E9%9D%9E%E6%A8%A1%E5%BC%8F%E5%AF%B9%E8%AF%9D%E6%A1%86.html</url>
    <content type="text"><![CDATA[应用实例:搜索框创建一个Widget对象，作为主窗体的成员变量SearchWindow* m_SearchWin;12m_SearchWin=new SearchWindow(this);m_SearchWin-&gt;setWindowFlags(Qt::Window); 在用户执行搜索菜单时显示此窗口12345int Text_7_4A::ShowSearch() &#123; m_searchWin-&gt;show(); return 0;&#125; 在搜索窗口内有动作时，在主窗口中相应此动作需要将搜索类的ui成员变为public类型12//响应小窗口内的动作 connect(m_searchWin-&gt;ui.btnStartSearch, SIGNAL(clicked()), this, SLOT(OnStartSearch()) ); //响应小窗口内的动作 connect(m_searchWin-&gt;ui.btnStartSearch, SIGNAL(clicked()), this, SLOT(OnStartSearch()) ); 笔芯❤️]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>对话框</tag>
        <tag>非模式对话框</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt-模式对话框]]></title>
    <url>%2FQt-%E6%A8%A1%E5%BC%8F%E5%AF%B9%E8%AF%9D%E6%A1%86.html</url>
    <content type="text"><![CDATA[QDialog类参考 对话框：用于提示用户输入 ####对话框分两种： （1）模式对话框Modal背景界面卡住，用户必须完成对话框输入，关闭对话框之后，才能回到背景界面 （2）非模式对话框Non‐Modal背景界面可以活动。 任务：点登录按钮，弹出对话框提示用户输入。用户点“确认”或“取消”后，回到主界面。 (1)新建对话框，布局(2)点确定时，accept() 对话框返回点取消时，reject()对话框返回12345678910111213int LoginDlg::OnbtnOK() &#123; //取得用户的输入 m_user = ui.TextUser-&gt;text(); m_password = ui.TextPassword-&gt;text(); accept();//关闭对话框，并返回Accepted return 0;&#125;int LoginDlg::OnbtnCancel() &#123; //用户取消 reject(); return 0;&#125; (3) 运行对话框 exec()，注意exec()的返回值(4) 取得用户输入123456789101112int QtLog_01::OnbtnOK_1() &#123; //使用对话框 LoginDlg dlg; int ret=dlg.exec();//对话框显示，程序阻塞 if (ret == QDialog::Accepted) &#123; qDebug()&lt;&lt; dlg.m_user &lt;&lt; dlg.m_password;//调试输出 &#125; else &#123; &#125; return 0;&#125; 源码下载 ❤️]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>对话框</tag>
        <tag>模式对话框</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt-文件对话框]]></title>
    <url>%2FQt-%E6%96%87%E4%BB%B6%E5%AF%B9%E8%AF%9D%E6%A1%86.html</url>
    <content type="text"><![CDATA[QFileDialog类参考 选择打开文件12345678910111213141516171819202122232425262728293031323334353637int QtFile_01::OnClickbtnOpen() &#123; //选择要打开的文件 QString fileName = QFileDialog::getOpenFileName(this, GBK::ToUnicode(&quot;Open File打开文件&quot;),//标题 &quot;&quot;, tr(&quot;(*)&quot;));//限制文件类型 //为空时表明用户取消了操作 if (fileName.length() &gt; 0) &#123; qDebug() &lt;&lt; fileName; //Unicode(QString) -&gt; GBK(string) string gbk_name = GBK::FromUnicode(fileName); //打开文件,读取内容 FILE* fp; fopen_s(&amp;fp, gbk_name.c_str(), &quot;rb&quot;); //获取文件大小 fseek(fp, 0, SEEK_END); int filesize = ftell(fp); //读取内容 fseek(fp, 0, SEEK_SET); char* buf = new char[filesize + 1]; int n = fread(buf,1, filesize, fp); if (n &gt; 0) &#123; buf[n] = 0; //显示文本 ui.plainTextEdit-&gt;setPlainText(GBK::ToUnicode(buf)); &#125; //释放内存，关闭界面 delete[]buf; fclose(fp); &#125; return 0;&#125; 保存文件123456789101112131415161718192021222324252627int QtFile_01::OnClickbtnSave() &#123; //选择要保存的位置 QString fileName = QFileDialog::getSaveFileName(this, //父窗口 GBK::ToUnicode(&quot;保存文件&quot;), &quot;/&quot;); //若为空表明用户取消了操作 if (fileName.length() &gt; 0) &#123; //获取文本框中的内容 QString text = ui.plainTextEdit-&gt;toPlainText(); //Unicode(QString) -&gt; GBK(string) string gbk_name = GBK::FromUnicode(fileName); string gbk_text = GBK::FromUnicode(text); //打开文件 FILE* fp; fopen_s(&amp;fp, gbk_name.c_str(), &quot;wb&quot;); //写入文件 fwrite(gbk_text.c_str(), 1, gbk_text.length(), fp); //关闭文件 fclose(fp); &#125; return 0;&#125; 文中ToUnicode()及FromUnicode均为封装好的函数，详见VC下字符编码转换]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>对话框</tag>
        <tag>文件对话框</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt-QLabe显示文字和图片]]></title>
    <url>%2FQt-QLabeshowtextandimages.html</url>
    <content type="text"><![CDATA[QLabe Class参考文档 文字与图片的显示格式实际上是用Qt StyleSheet（样式表）设置的与css样式中的差不多 对齐方式 是否换行 设置边框 显示LOGO(显示图片) 添加图片资源 添加一个Labe用来显示图片 ❤️]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>QLabe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/ var disqus_config = function () {this.page.url = PAGE_URL; // Replace PAGE_URL with your page’s canonical URL variablethis.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page’s unique identifier variable}; (function() { // DON’T EDIT BELOW THIS LINEvar d = document, s = d.createElement(‘script’);s.src = ‘https://nicebluechai.disqus.com/embed.js&#39;;s.setAttribute(‘data-timestamp’, +new Date());(d.head || d.body).appendChild(s);})(); Please enable JavaScript to view the comments powered by Disqus. (function(){var appid = ‘cytu1mJqc’;var conf = ‘prod_724639f82a7493e5cc3377fc5c98c32b’;var width = window.innerWidth || document.documentElement.clientWidth;if (width &lt; 960) {window.document.write(‘&lt;\/script&gt;’); } else { var loadJs=function(d,a){var c=document.getElementsByTagName(“head”)[0]||document.head||document.documentElement;var b=document.createElement(“script”);b.setAttribute(“type”,”text/javascript”);b.setAttribute(“charset”,”UTF-8”);b.setAttribute(“src”,d);if(typeof a===”function”){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e===”loaded”||e===”complete”){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs(“http://changyan.sohu.com/upload/changyan.js&quot;,function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); I love you forever I love you forever 我爱你 pengloo53www.linux2me.com I love you forever David LevithanWide Awake]]></content>
      <categories>
        <category>第一篇</category>
        <category>Test</category>
      </categories>
      <tags>
        <tag>英文</tag>
      </tags>
  </entry>
</search>
